{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "from scipy.stats import skew, norm, probplot\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,6.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "from sklearn.base import TransformerMixin,BaseEstimator, RegressorMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Preprocessing \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer, PowerTransformer, OneHotEncoder\n",
    "\n",
    "# Anamoly detection\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Model reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Regressors \n",
    "from sklearn.linear_model import Ridge, ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Model selection and validation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_validate, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dat\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test_id = df_test['Id']\n",
    "\n",
    "# Remove ID\n",
    "df_train = df_train.drop('Id',1)\n",
    "df_test = df_test.drop('Id',1)\n",
    "\n",
    "\n",
    "#MSSubClass=The building class\n",
    "# df_train['MSSubClass'] = df_train['MSSubClass'].apply(str)\n",
    "# df_train['MSSubClass'] = df_train['MSSubClass'].apply(str)\n",
    "\n",
    "# #Changing OverallCond into a categorical variable\n",
    "# df_train['OverallCond'] = df_train['OverallCond'].astype(str)\n",
    "# df_test['OverallCond'] = df_test['OverallCond'].astype(str)\n",
    "\n",
    "\n",
    "#Year and month sold are transformed into categorical features.\n",
    "# df_train['YrSold'] = df_train['YrSold'].astype(str)\n",
    "# df_test['MoSold'] = df_test['MoSold'].astype(str)\n",
    "\n",
    "# df_test['YrSold'] = df_test['YrSold'].astype(str)\n",
    "# df_test['MoSold'] = df_test['MoSold'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find features\n",
    "numeric_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n",
    "categorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"SalePrice\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train[\"SalePrice\"], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df_train['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns.to_series().groupby(df_train.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name= df_train.columns[7]\n",
    "sns.scatterplot(x='TotalBsmtSF',y='SalePrice',data=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='OverallQual',y='SalePrice',data=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.boxplot(x='Neighborhood',y='SalePrice',data=df_train)\n",
    "g = g.set_xticklabels(g.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(30.7,6.27)})\n",
    "g = sns.boxplot(x='YearBuilt',y='SalePrice',data=df_train)\n",
    "g = g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "sns.set(rc={'figure.figsize':(11.7,6.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "#correlation matrix\n",
    "corrmat = df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat.loc[corrmat['SalePrice']>0.5,'SalePrice'][:-1].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the features near diameter. TotalBsmtSF/FirstFlrSQ and GarageCars/GarageArea are highly correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(df_train[cols], height = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anamoly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination = 0.005 # Ratio of contaminated data\n",
    "anomaly_algorithms = {\n",
    "    #\"Robust covariance\": EllipticEnvelope(contamination=contamination),\n",
    "    \"Isolation Forest\": IsolationForest(behaviour='new', contamination=contamination,random_state=60),\n",
    "    #\"Local Outlier Factor\": LocalOutlierFactor(contamination=contamination)        \n",
    "}\n",
    "\n",
    "\n",
    "variables = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt','LotArea']\n",
    "outliers = dict()\n",
    "for i, var in enumerate(['Total']):\n",
    "    #val = df_train[var].values.reshape(-1,1)\n",
    "    val = df_train[variables]\n",
    "    for name, algorithm in anomaly_algorithms.items():\n",
    "        # fit the data and tag outliers\n",
    "        if name == \"Local Outlier Factor\":\n",
    "            y_pred = algorithm.fit_predict(val)\n",
    "        else:\n",
    "            y_pred = algorithm.fit(val).predict(val)\n",
    "                    \n",
    "        outliers[var] = y_pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot outliers\n",
    "n_var = len(variables)\n",
    "n_col = 4\n",
    "fig, axarr = plt.subplots(math.ceil(n_var/n_col), n_col, figsize=(25, 8))\n",
    "if (axarr.ndim==1):axarr = np.reshape(axarr, (-1, 2))     \n",
    "for i, var in enumerate(variables):    \n",
    "    outlier = pd.Series(outliers['Total']).map({1: 'normal', -1: 'outlier'}).astype('category')\n",
    "    _ = sns.scatterplot(ax = axarr[i//n_col][i%n_col], x = df_train[var], y=df_train['SalePrice'], hue = outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "df_train_clean = df_train.drop(index=np.where(outliers['Total']==-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(outliers['Total']==-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total =df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with missing data\n",
    "df_train_tidy = df_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\n",
    "df_train_tidy = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\n",
    "df_train_tidy.isnull().sum().max() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skweness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform skewed numeric features:\n",
    "skewed_feats = df_train_tidy[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75].index\n",
    "df_train_tidy[skewed_feats] = np.log1p(df_train_tidy[skewed_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train_tidy['SalePrice'],fit=norm)\n",
    "fig = plt.figure()\n",
    "res = probplot(df_train['SalePrice'],plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train_tidy['GrLivArea'],fit=norm)\n",
    "fig = plt.figure()\n",
    "res = probplot(df_train_tidy['GrLivArea'],plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train['TotalBsmtSF'],fit=norm)\n",
    "fig = plt.figure()\n",
    "res = probplot(df_train['TotalBsmtSF'],plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Pre-processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "        try:\n",
    "            return X[self.columns]\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)\n",
    "            \n",
    "#cs = ColumnSelector(columns=[\"state\", \"account length\", \"area code\"])\n",
    "#cs.fit_transform(df).head()\n",
    "\n",
    "class SkewCorrection(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, limit=0.75):\n",
    "        self.limit = limit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.idx = skew(X)>self.limit\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[:,self.idx] = np.log1p(X[:,self.idx]) # Using log1p to account for 0 inputs\n",
    "        return X\n",
    "    \n",
    "    \n",
    "class RemoveOutliers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, variables=['GrLivArea']):\n",
    "        self.variables = variables\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pi = Pipeline(steps=[\n",
    "    ('skew', SkewCorrection(limit=0.75))])\n",
    "a = pi.fit_transform(df_train[numeric_feats])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "# Considered\n",
    "# ('nonlinear', PowerTransformer(method='yeo-johnson', standardize=False)),\n",
    "# ('pca',PCA())\n",
    "\n",
    "numeric_features = numeric_feats.drop('SalePrice')#['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt','YearRemodAdd','GarageYrBlt','TotRmsAbvGrd']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('skew', SkewCorrection(limit=0.75)),\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "categorical_features = categorical_feats\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', Ridge())])\n",
    "\n",
    "x = df_train_clean.drop('SalePrice', axis=1)\n",
    "y = df_train_clean['SalePrice']\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "print(\"model R2 score: %.3f\" % pipeline.score(x_valid, y_valid))\n",
    "print(\"RMSE: %.3f\" % np.sqrt(mean_squared_error(y_valid,pipeline.predict(x_valid))))\n",
    "\n",
    "kfold = 5\n",
    "scoring = ['r2','neg_mean_squared_error']\n",
    "results = cross_validate(pipeline, x, y, cv=kfold, return_train_score=True,scoring=scoring)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cv(pipeline,x,y, kfold=5, scoring = ['r2','neg_mean_squared_error']):\n",
    "    results = cross_validate(pipeline, x, y, cv=kfold, \n",
    "                             return_train_score=True, return_estimator=False, \n",
    "                             scoring=scoring,\n",
    "                             n_jobs=-1) # Change the n_jobs to -1 to use all the cpus for calculation\n",
    "    \n",
    "    print('PL: {:<15} | test_score: {:1.4f} | train_score: {:1.4f} |\\\n",
    "    test_rmse: {:.4f} | fit_time: {:2.4f} | score_time: {:1.4f}'.format(name,\n",
    "                                                                        np.mean(results['test_r2']),\n",
    "                                                                        np.mean(results['train_r2']),\n",
    "                                                                        np.sqrt(-np.mean(results['test_neg_mean_squared_error'])),                                                      \n",
    "                                                                        np.mean(results['fit_time']),\n",
    "                                                                        np.mean(results['score_time'])))  \n",
    "    return True\n",
    "\n",
    "regressors = {\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(alpha=0.1,max_iter=1000,tol=0.0001),\n",
    "    'ElasticNet': ElasticNet(alpha=0.05, l1_ratio=.9, random_state=3),\n",
    "    'KernelRidge': KernelRidge(),\n",
    "    'GradBoost': GradientBoostingRegressor(),\n",
    "    'RandForest': RandomForestRegressor()\n",
    "}\n",
    "\n",
    "\n",
    "for name, reg in regressors.items():\n",
    "    steps=[('preprocessor', preprocessor),\n",
    "           ('regressor', reg)]\n",
    "    pipeline = Pipeline(steps)\n",
    "    model_cv(pipeline,x,y)\n",
    "\n",
    "print('Done!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning\n",
    "We choose Gradient boosting algorithm here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[('preprocessor', preprocessor),\n",
    "       ('regressor', GradientBoostingRegressor())]\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.steps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {\n",
    "    'Ridge': Ridge(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'GradBoost': GradientBoostingRegressor(),\n",
    "}\n",
    "        \n",
    "param_grid = {\n",
    "    'Ridge':{\n",
    "        'regressor__alpha': [0.001,0.01,0.1],\n",
    "    },\n",
    "    \n",
    "    'ElasticNet':{\n",
    "        'regressor__alpha': [0.001,0.01,0.1],\n",
    "        'regressor__l1_ratio': [0.1,0.3,0.9],\n",
    "        'regressor__random_state': [1,3,9]           \n",
    "    },\n",
    "    \n",
    "    'GradBoost':{\n",
    "        'preprocessor__num__imputer__strategy': ['mean', 'median'],    \n",
    "        'regressor__alpha': [0.001,0.01,0.1],\n",
    "        'regressor__n_estimators': [300,400],\n",
    "        'regressor__max_depth': [3,6]        \n",
    "    }\n",
    "}\n",
    "\n",
    "best_estimators = dict()\n",
    "\n",
    "# Loop over specified estimators and find the best parameters\n",
    "n_iter_search = 20\n",
    "for name, reg in regressors.items():\n",
    "    steps=[('preprocessor', preprocessor),\n",
    "           ('regressor', reg)]\n",
    "    pipeline = Pipeline(steps)\n",
    "    # grid = GridSearchCV(pipeline, param_grid[name], cv=5, n_jobs=-1, iid=False)\n",
    "    \n",
    "    # The result in parameter settings is quite similar, while the run time for randomized search is drastically lower. \n",
    "    # The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.\n",
    "    grid = RandomizedSearchCV(pipeline, param_grid[name], cv=5, n_jobs=-1, iid=False, n_iter=n_iter_search,)\n",
    "\n",
    "    grid.fit(x,y)\n",
    "\n",
    "    best_estimators[name] = grid.best_estimator_\n",
    "    \n",
    "    # summarize results\n",
    "    print('================ Regressor: {:<10} ================'.format(name))\n",
    "    print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "    means = grid.cv_results_['mean_test_score']\n",
    "    stds = grid.cv_results_['std_test_score']\n",
    "    params = grid.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking the regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "for name, est in best_estimators.items():\n",
    "    predictions[name]= est.predict(x)\n",
    "    #model_cv(est,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('regressor', LinearRegression())])\n",
    "    \n",
    "model_cv(pipeline,predictions,y)\n",
    "#stacked_estimator.fit(predictions, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedModel(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        predictions = pd.DataFrame()\n",
    "        for name, est in best_estimators.items():\n",
    "            predictions[name]= est.predict(X)\n",
    "            \n",
    "        self.pipeline = Pipeline([('regressor', LinearRegression())])\n",
    "        self.pipeline.fit(predictions,y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = pd.DataFrame()\n",
    "        for name, est in best_estimators.items():\n",
    "            predictions[name]= est.predict(X)\n",
    "            \n",
    "        return self.pipeline.predict(predictions)\n",
    "    \n",
    "stackModel = StackedModel(best_estimators)    \n",
    "stackModel.fit(x_train,y_train)\n",
    "r2_score(y_valid,stackModel.predict(x_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot importatnt features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot important coefficients\n",
    "coefs = pd.Series(clf.steps[1][1].coef_, index = X_train.columns)\n",
    "print(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n",
    "      str(sum(coefs == 0)) + \" features\")\n",
    "imp_coefs = pd.concat([coefs.sort_values().head(10),\n",
    "                     coefs.sort_values().tail(10)])\n",
    "imp_coefs.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Ridge Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('nonlinear', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
    "    ('scaler', StandardScaler())])\n",
    "pip1 = numeric_transformer.fit(df_train[['OverallQual', 'GrLivArea']], df_train['SalePrice'])\n",
    "res = pip1.transform(df_train[['OverallQual', 'GrLivArea']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train['TotalBsmtSF'],fit=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(res[:,1],fit=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stackModel.predict(df_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib \n",
    "\n",
    "joblib.dump(clf, 'my_regressor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'Id': df_test_id, 'SalePrice': y_pred})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
